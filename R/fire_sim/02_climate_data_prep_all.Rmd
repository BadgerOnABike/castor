---
title: "02_climate_data_prep_all"
author: "Elizabeth Kleynhans and Cora Skaien"
date: "07/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<!--
Copyright 2021 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
-->

#===================

This script is adapted from an earlier script. The initial script separates lightning caused fires before subsampling, and this one is updated to subsample before human and lightning caused fires (and those of unknown origin) are separated, to ensure that "no fire" locations do not occur where a fire occurred with another designation (lightning, human, unknown).

Script Purpose: This script obtains the lat, long coordinates of fire ignition locations and samples locations where fires were not observed to start. It then creates a csv file with these locations that can be used to manually extract monthly average climate variables from climateBC (http://climatebc.ca/) for all years 2002 to 2020. This range of dates was chosen because it is the years that we have VRI data for. To extract the climate data I use the app that climateBC provides. The version I used of the app is climateBC_v710. This version was released on 06 June 2021 and includes 13 General Circulation Models from the CMIP6. It also has a different normal period (1991 - 2020).  After the climate data has been extracted from climateBC this data is reimported into this script and the mean monthly drought code for the months  May - September is calculated for each year. From this script I get the maximum temperature, minimum temperature, average temperature, total precipitation, and mean monthly drought code for the months May - September for each year 2002 - 2020 for all fire ignition locations and randomly sampled (available fire ignition locations (fire absence)) points on the landscape 
#  Script Author: Elizabeth Kleynhans, Ecological Modeling Specialist, Forest Analysis and Inventory Branch, B.C. Ministry of Forests, Lands, and Natural Resource Operations.
#  Script Contributor: Cora Skaien, Ecological Modeling Specialist, Forest Analysis and Inventory Branch, B.C. Ministry of Forests, Lands, and Natural Resource Operations.
#=================================


#Overview:
In this file, we determine the BEC boundaries and match this with our fire location data within the BC boundary, create 500 m buffers around points, select GPS locations where fires did not start, combine fire and non-fire location data, acquire the lat and long data to get ClimateBC information for each location, calculate the monthly drought code for each dataset, and then upload files to clus database.

```{r}

library(raster)
library(data.table)
library(sf)
library(tidyverse)
library(rgeos)
library(cleangeo)
library(dplyr)
library(tidyr)
library(ggplot2)


source(here::here("R/functions/R_Postgres.R"))
```

Now we must bring in the relevant data

```{r}

#Get BEC data
bec<-getSpatialQuery("SELECT objectid, feature_class_skey, zone, subzone, natural_disturbance, zone_name, wkb_geometry FROM public.bec_zone")
st_crs(bec)

bec<-st_transform(bec, 3005) #transform coordinate system to 3005, which refers to BC, Canada
# EPSG:3005 Projected coordinate system for Canada - British Columbia. This CRS name may sometimes be used as an alias for NAD83(CSRS) / BC Albers.
#plot(bec[, "zone"]) # check we got the whole province
st_crs(bec)

# import fire ignition data
fire.ignition<-getSpatialQuery("SELECT * FROM public.bc_fire_ignition")# this file has historic and current year fires joined together. 
st_crs(fire.ignition)
fire.ignition<-st_transform(fire.ignition, 3005) #transform coordinate system to 3005 - that for BC, Canada

prov.bnd <- st_read ( dsn = "T:\\FOR\\VIC\\HTS\\ANA\\PROJECTS\\CLUS\\Data\\admin_boundaries\\province\\gpr_000b11a_e.shp", stringsAsFactors = T) # Read simple features from file or database, or retrieve layer names and their geometry type(s)
st_crs(prov.bnd) #Retrieve coordinate reference system from sf or sfc object
prov.bnd <- prov.bnd [prov.bnd$PRENAME == "British Columbia", ] 
bc.bnd <- st_transform (prov.bnd, 3005) #Transform coordinate system
fire.ignition.clipped<-fire.ignition[bc.bnd,] # making sure all fire ignitions have coordinates within BC boundary
table(fire.ignition.clipped$fire_year) #Still have correct number for 2007 and 2008

st_write(fire.ignition.clipped, overwrite = TRUE,  dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\bc_fire_ignition_clipped.shp", delete_dsn = TRUE)
##Check clipped data on QGis. Clipped data has no physical outliers
# note clipping the fire locations to the BC boundary removes a few ignition points in several of the years

bec_sf<-st_as_sf(bec) #Convert foreign object to an sf object (collection of simple features that includes attributes and geometries in the form of a data frame. In other words, it is a data frame (or tibble) with rows of features, columns of attributes, and a special geometry column that contains the spatial aspects of the features)
bec_sf_buf<- st_buffer(bec_sf, 0) # encircles a geometry object at a specified distance and returns a geometry object that is the buffer that surrounds the source object. Set buffer to 0 here.
st_crs(bec_sf_buf)
names(bec_sf_buf)
st_write(bec_sf_buf, overwrite = TRUE,  dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\bec_sf_buf.shp", delete_dsn = TRUE)
#Boundaries look good


fire.ignition_sf<-st_as_sf(fire.ignition.clipped) #convert to sf object
st_crs(fire.ignition_sf)
st_write(fire.ignition_sf, overwrite = TRUE,  dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\bc_fire_ignition_sf.shp", delete_dsn = TRUE)
#Boundaries look good
table(fire.ignition_sf$fire_year)

```

Note, that if you bring this BEC Buffer data back in, the names may have been shortened given column name limitations for shape files. You may need to run the below chunk to rename appropriately, but remember you may need to REMOVE THE PLYR PACKAGE AFTER! This is because of conflicts with other packages for language. If you are continuing from the code chunk above, then no need to run the below chunk as you will already have the file ready to go in your R environment.

This may occur for the fire.ignition_sf file as well. So check the naming and update as necessary.

```{r}

##If you bring BEC buffer back in, you may need to do the below for renaming. Remember to open the plyr library, but then ensure it is closed in the packages afterwards

#And we need BEC buffer info
bec_sf_buf<-st_read(dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\bec_sf_buf.shp")
names(bec_sf_buf)

#Must rename some variables because it was shortened when saved as a shape file
library(plyr)
names(bec_sf_buf)
bec_sf_buf<-rename(bec_sf_buf,
                   c("objectd"="objectid",
                     "ftr_cl_"="feature_class_skey",
                     "ntrl_ds"="natural_disturbance",
                     "zone_nm"="zone_name"))
names(bec_sf_buf)
## NOW UNCHECK PLYR LIBRARY IN PACKAGES WINDOW!
```

The below code chunk is VERY slow! So be prepared to go do other work for a while. Once it is complete, save the file and avoid doing this step again unless necessary.

```{r}

#doing st_intersection with the whole bec and fire ignition files is very very slow! 

fire.igni.bec<- st_intersection(fire.ignition_sf, bec_sf_buf) # ST_Intersects is a function that takes two geometries and returns true if any part of those geometries is shared between the 2
#fire.igni.bec<- st_intersection(fire.ignition_sf, st_buffer(bec_sf, 0)) #Alternative code
# ST_Intersection in conjunction with ST_Intersects is very useful for clipping geometries such as in bounding box, buffer, region queries where you only want to return that portion of a geometry that sits in a country or region of interest
table(fire.igni.bec$fire_year)
```

Write fire.igni.bec to file because it takes so long to make.

```{r}
st_write(fire.igni.bec, overwrite = TRUE,  dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\fire_ignit_by_bec.shp", delete_dsn = TRUE)

##Save via OsGeo4W Shell
##Below needs: (1) update to relevant credentials and (2) then enter into the OSGeo4W command line and hit enter. 
#ogr2ogr -f PostgreSQL PG:"host=localhost user=postgres dbname=postgres password=postgres port=5432" C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\fire_ignit_by_bec.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

```

Run the below code chunk to bring the data back in if you are starting from this portion of the code. This way, you can avoid losing time running the above again if you have already created it.

```{r}

#import the fire ignition data
##Can use keyring
conn <- DBI::dbConnect (dbDriver ("PostgreSQL"), 
                        host = keyring::key_get('dbhost', keyring = 'postgreSQL'), 
                        dbname = keyring::key_get('dbname', keyring = 'postgreSQL'), 
                        port = '5432',
                        user = keyring::key_get('dbuser', keyring = 'postgreSQL'),
                        password = keyring::key_get('dbpass', keyring = 'postgreSQL'))

fire_igni_bec<- st_read (dsn = conn, 
          layer = c ("public", "fire_ignit_by_bec"))
dbDisconnect (conn)

##Or from local device; e.g. below
# fire_igni_bec <-st.read(dsn="C:\\Work\\caribou\\clus_data\\Fire\\Fire_sim_data\\fire_ignition_hist\\fire_ignit_by_bec.shp")

head(fire_igni_bec)
```

Now we will get the lat-long into separate columns for processing downstream.

```{r}

#getting long lat info
#geo.prj <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
fire_igni_bec1 <- st_transform(fire_igni_bec, crs = "+proj=longlat +datum=NAD83 / BC Albers +no_defs")
st_crs(fire_igni_bec1) #Retrieve coordinate reference system to check
fire_igni_bec2<-as.data.frame(fire_igni_bec1)
fire_igni_bec2<-fire_igni_bec2 %>% 
  dplyr::select(ogc_fid: fire_type, size_ha:wkb_geometry)
# Try find a way to split the data up into 3 columns and then remove the brackets. 
fire_igni_bec3<- fire_igni_bec2 %>%
  tidyr::separate(wkb_geometry, into = c("longitude", "latitude")," ")
fire_igni_bec3$longitude<- gsub(",", "", as.character(fire_igni_bec3$longitude) )
fire_igni_bec3$longitude<- substring(fire_igni_bec3$longitude, 3)
fire_igni_bec3$longitude<- as.numeric(fire_igni_bec3$longitude)
fire_igni_bec3$longitude<- round(fire_igni_bec3$longitude, digits=4)
fire_igni_bec3$latitude<- gsub(")", "", as.character(fire_igni_bec3$latitude) )
fire_igni_bec3$latitude<- as.numeric(fire_igni_bec3$latitude)
fire_igni_bec3$latitude<- round(fire_igni_bec3$latitude, digits=4)

fire_igni_bec_new<-fire_igni_bec %>% dplyr::select(ogc_fid: fire_type, size_ha: wkb_geometry)

fire_igni_bec_new$longitude<-fire_igni_bec3$longitude
fire_igni_bec_new$latitude<-fire_igni_bec3$latitude
```

Now let's buffer each fire location by 500m, and within each BEC Zone, we'll sample locations where fires did not start and combine those locations with locations where the fires did start. In the initial code, this was done for only lightning caused fires. Here, we will do for all fires to avoid accidentally selecting a location where fires occurred for another cause and have it accidentally designated as an area without fire.

Buffer is so that the areas selected as non-fire locations are less likely to have been fire affected.

Each time you run the loop below, you will get new coordinates for areas not affected by fire. As a result, you will need to re-run the ClimateBC stuff manually each time you run the below chunk. Once you have ran it once, and acquired ClimateBC data for the locations, you can avoid running it again by saving the file and starting on the next script from that file onward.

This loop will take a while to run, so have other work plans while it is running.

```{r}
years<-c("2002", "2003", "2004", "2005", "2006", "2007","2008","2009","2010","2011","2012","2013","2014","2015","2016","2017","2018", "2019", "2020")
beczone<- c("SWB", "SBS", "ICH", "ESSF", "MH", "CWH", "BAFA", "CMA", "SBPS", "IMA", "MS", "PP", "IDF", "BWBS", "BG", "CDF")
filenames<-list()

##Run loop below. This loop will take some time.

for (i in 1:length(years)) {
  print(years[i])
  foo<- fire_igni_bec_new %>% filter(fire_year==years[i])
  foo_ignit_sf<- st_as_sf(foo)
  
  all_sample_points <- data.frame (matrix (ncol = 18, nrow = 0)) # add 'data' to the points
  colnames (all_sample_points) <- c ("ogc_fid", "fire_no", "fire_year", "ign_date", "fire_cause", "fire_id", "fire_type", "latitude", "longitude", "size_ha", "objectid", "feature_class_skey", "zone", "subzone", "natural_disturbance", "zone_name", "fire", "wkb_geometry")
  
  for (j in 1:length(beczone)) {
    print(beczone[j])
    
    foo_ignit_small<- foo_ignit_sf %>% filter(zone==beczone[j])
    
    if (dim(foo_ignit_small)[1]>0) {
    foo_ignit_small$fire<-1
    foo.ignit.buffered<- st_buffer(foo_ignit_small, dist=500) # buffering fire ignition locations by 500m. I decided to do this because I dont think the recorded locations are likely very accurate so I hope this helps
    foo.ignit.buffered<-foo.ignit.buffered %>% 
      dplyr::select(ogc_fid, fire_no, fire_id, zone, subzone, wkb_geometry)
    foo.ignit.buf.union<-st_union(foo.ignit.buffered)
    
    bec_foo<- bec_sf_buf %>% filter(zone==beczone[j])
    clipped<-st_difference(bec_foo, foo.ignit.buf.union)
    #clipped<-rmapshaper::ms_erase(target=bec_foo, erase=foo.ignit.buffered) # clips out buffered areas I think.But it crashes a lot!
    
    ##Below we sample 10x as many points for where fires do not occur 
    #sample_size<-dim(foo_ignit_small)[1]*10 # here 10 is the number of points I sample in correlation with the number of ignition points in that BEC zone. 
    sample_size<-dim(foo_ignit_small)[1]*5 # here 5 is the number of points I sample in correlation with the number of ignition points in that BEC zone.
    samp_points <- st_sample(clipped, size=sample_size)
    samp_points_sf = st_sf(samp_points)
    samp_joined = st_join(samp_points_sf, clipped) # joining attributes back to the sample points
    samp_joined<- st_transform(samp_joined, 3005)
    samp_joined$ogc_fid<-"NA"
    samp_joined$fire_no<-"NA"
    samp_joined$fire_year<- years[i]
    samp_joined$ign_date<-"NA"
    samp_joined$fire_cause<-"NA"
    samp_joined$fire_id<-"NA"
    samp_joined$fire_type<-"NA"
    samp_joined$size_ha<-"NA"
    samp_joined$fire<-0
    
    #getting long lat info
    #geo.prj <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
    sample.p.trans <- st_transform(samp_joined, crs = "+proj=longlat +datum=NAD83 / BC Albers +no_defs")
    #st_crs(sample.p.trans)
    sample.p.trans1<-as.data.frame(sample.p.trans)
    # Try find a way to split the data up into 3 colums and the remove the brackets. 
    samp_joined2<- sample.p.trans1 %>%
      tidyr::separate(geometry, into = c("longitude", "latitude")," ")
    samp_joined2$longitude<- gsub(",", "", as.character(samp_joined2$longitude) )
    samp_joined2$longitude<- substring(samp_joined2$longitude, 3)
    samp_joined2$longitude<- as.numeric(samp_joined2$longitude)
    samp_joined2$longitude<- round(samp_joined2$longitude, digits=4)
    samp_joined2$latitude<- gsub(")", "", as.character(samp_joined2$latitude) )
    samp_joined2$latitude<- as.numeric(samp_joined2$latitude)
    samp_joined2$latitude<- round(samp_joined2$latitude, digits=4)
    
    samp_joined$longitude<-samp_joined2$longitude
    samp_joined$latitude<-samp_joined2$latitude
    samp_joined_new<- samp_joined %>% 
      rename(wkb_geometry=geometry) %>%
      dplyr::select(ogc_fid, fire_no, fire_year, ign_date, fire_cause, fire_id, fire_type, latitude, longitude, size_ha, objectid, feature_class_skey, zone, subzone, natural_disturbance,zone_name, wkb_geometry, fire)
    
    pnts<- rbind(samp_joined_new, foo_ignit_small)
    
    all_sample_points<- rbind(all_sample_points, pnts)
    
    
    } 
    
  }
  
  #assign file names to the work
  nam1<-paste("sampled_points",years[i],sep="_") #defining the name
  assign(nam1,all_sample_points)
  filenames<-append(filenames,nam1)
}



```
In the above code, when ran for just lightning fires (see old code in "old" folder), we do not get the following error. However, when not filtering by lightning, we get this error for every BEC zone:
"attribute variables are assumed to be spatially constant throughout all geometries". I am unsure if this is of concern. This is odd, as this typically arises without an st_intersect, which we have performed between the layers being used (or were part of the steps to make the layer we are using). We have done the intersect and are using that resultant file, so I am unsure why this is occurring, but suspect that we can ignore the warning. Something to investigate perhaps, however.

Now that we have completed the loop, save the output as an R Object.

```{r}

mkFrameList <- function(nfiles) {
  d <- lapply(seq_len(nfiles),function(i) {
    eval(parse(text=filenames[i]))
  })
  do.call(rbind,d)
}

n<-length(filenames)
samp_locations<-mkFrameList(n) 
samp_locations$idno<-1:length(samp_locations$fire_year)
samp_locations_sf<-st_as_sf(samp_locations)
st_crs(samp_locations_sf)
head(samp_locations_sf) #Note, wkb_geometry is in different coordinate system for this data
table(is.na(samp_locations_sf$ogc_fid))

##Check data
table(samp_locations_sf$fire_cause) 
table(samp_locations_sf$fire_type) 
table(samp_locations_sf$subzone)
table(samp_locations_sf$fire_year) 

table(samp_locations_sf$fire_year, samp_locations_sf$fire_cause) # We see that we have oue 3 original categories, but now we also have our NA locations.
```

Now we must save this file locally and on clus so that we do not need to create it again.

```{r}
# or save it as a shape file
st_write(samp_locations_sf, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\samp_locations_fire_all_5x.shp", delete_dsn = TRUE, overwrite = TRUE)

table(samp_locations_sf$fire_year, samp_locations_sf$fire_type) 

#Because it is a shape file, we will need OsGeo4W Shell to save it to clus
##Below needs: (1) update to relevant credentials and (2) then enter into the OSGeo4W command line and hit enter. 
#ogr2ogr -f PostgreSQL PG:"host=localhost user=postgres dbname=postgres password=postgres port=5432" D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\samp_locations_fire_all_5x.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI


```

########### Acquiring and Appending Climate Data ########

First, we will generate separate files from above code for each year to input into ClimateBC.

```{r}
for (i in 1: length(years)) {
  dat<- samp_locations_sf %>% filter(fire_year==years[i])
  sample.pts.all <- data.frame (matrix (ncol = 5, nrow = nrow (dat)))
  colnames (sample.pts.all) <- c ("ID1","ID2", "lat", "long", "el")
  sample.pts.all$ID1<- dat$idno
  sample.pts.all$ID2 <- dat$fire_year
  sample.pts.all$lat <-as.numeric(dat$latitude)
  sample.pts.all$long <- as.numeric(dat$longitude)
  sample.pts.all$el <- "."
  
  nam1<-paste("sampled.points.all",years[i], "csv",sep=".")
  the_dir <- "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data"
  write.csv(sample.pts.all, file = paste0(the_dir, "\\", basename(nam1)), row.names=FALSE)
}

```

This next step needs to be conducted outside of R and then the resultant files can be brought back in to R.

Each time the code in this file is run up to this point, below will need to be repeated.

Next, see http://climatebc.ca/Help for how to use ClimateBC to get the climate data. 
You will need to download ClimateBC (http://climatebc.ca/downloads/download.html) and use the files you generate as input to the first code chunk below.

##To acquire ClimateBC Data for your sampled Locations##
1. Open ClimateBC on your computer.
2. In the Multi-Location section, select "Annual Data" and select the appropriate year for each individual file. 
3. In the bottom drop down menu, select "monthly primary variables". 
4. Upload each year, one at a time, and specify an output file location. Name the output files as the default suggested, but create a folder specifically for your outputs for each run.
5. Once all things are set up, click the "Start" button
6. Repeat for each year

Once the above has been completed for the random points and known fire locations you have created in code earlier in this R Markdown file, proceed to the next code chunk.

```{r}

###############################
#Import climate data per ignition and sample location
###############################

#Depending on where you saved your output, you may need to update the directory below
file.list1<-list.files("D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\output_July2021_5x", pattern="sampled.points.all", all.files=FALSE, full.names=FALSE)
y1<-gsub(".csv","",file.list1)
the_dir <- "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\output_July2021_5x"

for (i in 1:length(file.list1)){
  assign(paste0(y1[i]),read.csv (file=paste0(the_dir, "\\", file.list1[i])))
}



```

Because much literature suggests that the monthly drought code (MDC) is an important factor, we will use information acquired from ClimateBC to get MDC values.

```{r}
# FOR EACH DATASET CALCULATE THE MONTHLY DROUGHT CODE

#############################################
#### Equations to calculate drought code ####
#############################################

days_month<- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31) # number of days in each month starting in Jan
#### Daylength adjustment factor (Lf) [Development and Structure of the Canadian Forest Fire Weather Index System pg 15, https://d1ied5g1xfgpx8.cloudfront.net/pdfs/19927.pdf] ####
# Month <- Lf value
# LF[1] is the value for Jan
Lf<-c(-1.6, -1.6, -1.6, 0.9, 3.8, 5.8, 6.4, 5.0, 2.4, 0.4, -1.6, -1.6)
####

### Calculate drought code for Fire ignition data
filenames<-list()
for (i in 1: length(y1)){
  
  x<-eval(as.name(y1[i])) %>% 
    rename(YEAR=ID2) %>%
    dplyr::select(ID1, YEAR,Latitude, Longitude, Tmax05:Tmax09, Tave05:Tave09, PPT05:PPT09)
  
  x2<- x %>% filter(Tmax05 != -9999) # there are some locations that did not have climate data, probably because they were over the ocean, so Im removing these here.
  
  for (j in 5 : 9) {
    
    x2$MDC_04<-15
    
    Em<- days_month[j]*((0.36*x2[[paste0("Tmax0",j)]])+Lf[j])
    Em2 <- ifelse(Em<0, 0, Em)
    DC_half<- x2[[paste0("MDC_0",j-1)]] + (0.25 * Em2)
    precip<-x2[[paste0("PPT0",j)]]
    RMeff<-(0.83 * (x2[[paste0("PPT0",j)]]))
    Qmr<- (800 * exp((-(DC_half))/400)) + (3.937 * RMeff)
    Qmr2 <- ifelse(Qmr>800, 800, Qmr)
    MDC_m <- (400 * log(800/Qmr2)) + 0.25*Em2
    x2[[paste0("MDC_0",j)]] <- (x2[[paste0("MDC_0",j-1)]] + MDC_m)/2
    x2[[paste0("MDC_0",j)]] <- ifelse(x2[[paste0("MDC_0",j)]] <15, 15, x2[[paste0("MDC_0",j)]])
  }
  nam1<-paste("DC.",y1[i],sep="") #defining the name
  assign(nam1,x2)
  filenames<-append(filenames,nam1)
}

# combined all the DC.ignition files together
mkFrameList <- function(nfiles) {
  d <- lapply(seq_len(nfiles),function(i) {
    eval(parse(text=filenames[i]))
  })
  do.call(rbind,d)
}

n<-length(filenames)
DC.ignitions<-mkFrameList(n) 
DC.ignitions$ID1<- as.factor(DC.ignitions$ID1)

dim(DC.ignitions) 
names(DC.ignitions)
names(samp_locations_sf)

DC.ignitions1<- DC.ignitions %>% rename(idno=ID1,
                                        fire_year=YEAR) 
samp_locations_sf$idno <- as.factor(as.character(samp_locations_sf$idno))
samp_locations_sf$fire_year <- as.numeric(as.character(samp_locations_sf$fire_year))

# Now join DC.ignitions back with the original fire ignition dataset
ignition_weather<-left_join(DC.ignitions1, samp_locations_sf)
head(ignition_weather) #Lat -Longs match
dim(ignition_weather) 
st_crs(ignition_weather) #Answer NA
head(ignition_weather) #Note, there are 2 Lat/Long columns: ensure that they are the same; otherwise, you may be using the incorrect climate csvs that were manually created.
ignition_weather_crs <- st_as_sf(ignition_weather)
crs(ignition_weather_crs)
ignition_weather_crs<- st_transform(ignition_weather_crs, 3005)
crs(ignition_weather_crs)


```

We want to ensure that everything lines up with the BC Boundaries, so perform below

```{r}
# Check the points line up with BC boundaries!
ggplot() +
  geom_sf(data=bc.bnd, col='red') +
  geom_sf(data=ignition_weather_crs, col='black') #looks good
#If random points appear in middle of ocean, open in QGIS to get points and see what has happened.


# A check of the fire ignition counts per year line up with the original data. So the number of fire ignitions seem good.
```

Prepare ignition month information for later use. Note, locations without fires (and a few locations with fires too) will have NAs or blanks, and this is ok.

```{r}
ignition_weather_crs$ign_dat
ignition_weather_crs$ign_month<-stri_sub(ignition_weather_crs$ign_dat,5,6)
head(ignition_weather_crs)
ignition_weather_crs$ign_month
```


If data looks good, then save file.

```{r}

##Can use keyring
conn <- DBI::dbConnect (dbDriver ("PostgreSQL"), 
                        host = keyring::key_get('dbhost', keyring = 'postgreSQL'), 
                        dbname = keyring::key_get('dbname', keyring = 'postgreSQL'), 
                        port = '5432',
                        user = keyring::key_get('dbuser', keyring = 'postgreSQL'),
                        password = keyring::key_get('dbpass', keyring = 'postgreSQL'))


st_write (obj = ignition_weather_crs, 
          dsn = conn, 
          layer = c ("public", "DC_data_ALL_5x"))

dbDisconnect (conn)
```


```{r}

st_write(ignition_weather_crs, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\DC_data_5x.shp", delete_layer=TRUE)
##Can also open in QGis to assess for any physical outliers and their information.

#str(ignition_weather_crs)
#head(ignition_weather_crs)
#ignition_weather_crs<-st_as_sf(ignition_weather_crs)


##Below needs: (1) update to relevant credentials and (2) then enter into the OSGeo4W command line and hit enter. 
#ogr2ogr -f PostgreSQL PG:"host=DC052586 user= dbname=clus password= port=5432" D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\DC_data_5x.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI
##Above may not work because ogc_fid is NA or not right character type, and the code is trying to set this as the FID when uploading.

#key_get('dbpass', keyring = 'postgreSQL')

# OR my local machine

# ogr2ogr -f "PostgreSQL" PG:"host=localhost user=postgres dbname=postgres password=postgres port=5432" D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\DC_data_5x.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

```

Note, we lose a lot of data in 2002 when clipping with VRI later. Upon inspection, we can see that this is because there are empty spaces in VRi data for 2002. This will make more sense later, but if you would like to inspect this for yourself, create the 2002 points data now.

```{r}

##Get 2002 data and visualize in QGis
ignition_weather_crs_2002<-subset(ignition_weather_crs,ignition_weather_crs$fire_year==2002)
head(ignition_weather_crs_2002)
st_write(ignition_weather_crs_2002, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\DC_data_2002.shp", delete_layer=TRUE)

```



#########################################
#### FINISHED NOW GO TO 03_DEM_data_prep####
#########################################
