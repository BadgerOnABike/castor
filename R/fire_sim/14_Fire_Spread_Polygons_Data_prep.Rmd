---
title: "14_Fire_Spread_Polygons_Data_prep"
author: "Cora Skaien"
date: "16/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Load libraries
library(sf)
library(raster)
library(tidyverse)
library(rgeos)
library(cleangeo)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rgdal)
library(keyring)
library(DBI)
library(caret)
library(gdata)
library(stringi)

source(here::here("R/functions/R_Postgres.R"))
```

<!--
Copyright 2021 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
-->

#=================================
#  Script Name: 14_Fire_Spread_Polygons_Data_prep.R
#  Script Version: 1.0
#  Script Purpose: data prep for polygons of spread area for fires as part of spread.
#  Script Author: Cora Skaien, Ecological Modeling Specialist, Forest Analysis and Inventory Branch, B.C. Ministry of Forests, Lands, and Natural Resource Operations.
#=================================


#Overview
In this file, we will be creating a 500 m and 150 m buffers around each fire burn polygon, subtract the polygon area from the buffer, and then divide both the buffers and polygons into 1 ha squares (approximately). With this, we can attempt to create logistic regressions where the 1 ha squares from the buffers are 0 for spread and the 1 ha squares from the fire polygons receive a 1. The data will not be entirely related to environmental variables, however, as it will be confounded by fire-fighting efforts and fire weather change - but we will see if we can find any patterns using this approach. I suspect climate will not be able to come out from this, resulting in fire size being important for characterizing how climate impacts fires. Here, we can assess how topography, VRI and land-use impact the probability of spread into neighbouring cells. 

We will first load in the fire data, and separate the fire boundaries by year for 2002-2020. We will then bring each file into QGIS and create 150 and 500 m buffers.Then we can create the 1 ha squares and assign them 0/1 by clipping them to the fire and buffer polygons. We will not get ClimateBC data since the difference between adjacent cells will be minimal and we suspect this will influence fire size prior, but less likely spread. We will need to get VRI data for each 1 ha datapoint, including stand volume, age and height, and land-use (bclcs_5).

#Load in fire perimeter polygons
```{r}

fire_boundaries<-st_read(dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\H_FIRE_PLY_polygon.shp", stringsAsFactors = T)

st_crs(fire_boundaries)
fire_boundaries2 <- st_transform (fire_boundaries, 3005)
st_crs(fire_boundaries2) #Same as before

#Inspect
head(fire_boundaries2)

```

#Subset for 2002-2020
```{r}
fire_boundaries2002_20<-subset(fire_boundaries2, fire_boundaries2$FIRE_YEAR>2001)
table(fire_boundaries2002_20$FIRE_YEAR)

st_write(fire_boundaries2002_20, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_polygon_2002_20.shp", delete_dsn = TRUE)
#Lots of error messages with many features not successfully writen...
```

##Investigate years

If we want to get differences by buffers and polygons each year, it is best to separate by years so each polygon knows which other polygon to have a difference with.

```{r}
table(fire_boundaries2$FIRE_YEAR)
fire_boundaries2002_20<-subset(fire_boundaries2, fire_boundaries2$FIRE_YEAR>2001)
table(fire_boundaries2002_20$FIRE_YEAR)

fire_boundaries2002<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2002)
fire_boundaries2003<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2003)
fire_boundaries2004<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2004)
fire_boundaries2005<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2005)
fire_boundaries2006<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2006)
fire_boundaries2007<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2007)
fire_boundaries2008<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2008)
fire_boundaries2009<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2009)
fire_boundaries2010<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2010)
fire_boundaries2011<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2011)
fire_boundaries2012<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2012)
fire_boundaries2013<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2013)
fire_boundaries2014<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2014)
fire_boundaries2015<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2015)
fire_boundaries2016<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2016)
fire_boundaries2017<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2017)
fire_boundaries2018<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2018)
fire_boundaries2019<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2019)
fire_boundaries2020<-subset(fire_boundaries2002_20, fire_boundaries2002_20$FIRE_YEAR==2020)

```

```{r}

st_write(fire_boundaries2002, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2002.shp", delete_dsn = TRUE)

st_write(fire_boundaries2003, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2003.shp", delete_dsn = TRUE)

st_write(fire_boundaries2004, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2004.shp", delete_dsn = TRUE)

st_write(fire_boundaries2005, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2005.shp", delete_dsn = TRUE)

st_write(fire_boundaries2006, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2006.shp", delete_dsn = TRUE)

st_write(fire_boundaries2007, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2007.shp", delete_dsn = TRUE)

st_write(fire_boundaries2008, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2008.shp", delete_dsn = TRUE)

st_write(fire_boundaries2009, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2009.shp", delete_dsn = TRUE)

st_write(fire_boundaries2010, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2010.shp", delete_dsn = TRUE)

st_write(fire_boundaries2011, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2011.shp", delete_dsn = TRUE)

st_write(fire_boundaries2012, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2012.shp", delete_dsn = TRUE)

st_write(fire_boundaries2013, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2013.shp", delete_dsn = TRUE)

st_write(fire_boundaries2014, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2014.shp", delete_dsn = TRUE)

st_write(fire_boundaries2015, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2015.shp", delete_dsn = TRUE)

st_write(fire_boundaries2016, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2016.shp", delete_dsn = TRUE)

st_write(fire_boundaries2017, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2017.shp", delete_dsn = TRUE)

st_write(fire_boundaries2018, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2018.shp", delete_dsn = TRUE)

st_write(fire_boundaries2019, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2019.shp", delete_dsn = TRUE)

st_write(fire_boundaries2020, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_2020.shp", delete_dsn = TRUE)

```

##try 500 m buffer that we can do here or in QGIS

```{r}
fire_boundaries_2002_buf_500<- st_buffer(fire_boundaries2002, 500)
fire_boundaries_2003_buf_500<- st_buffer(fire_boundaries2003, 500)
fire_boundaries_2004_buf_500<- st_buffer(fire_boundaries2004, 500)
fire_boundaries_2005_buf_500<- st_buffer(fire_boundaries2005, 500)
fire_boundaries_2006_buf_500<- st_buffer(fire_boundaries2006, 500)
fire_boundaries_2007_buf_500<- st_buffer(fire_boundaries2007, 500)
fire_boundaries_2008_buf_500<- st_buffer(fire_boundaries2008, 500)
fire_boundaries_2009_buf_500<- st_buffer(fire_boundaries2009, 500)
fire_boundaries_2010_buf_500<- st_buffer(fire_boundaries2010, 500)
fire_boundaries_2011_buf_500<- st_buffer(fire_boundaries2011, 500)
fire_boundaries_2012_buf_500<- st_buffer(fire_boundaries2012, 500)
fire_boundaries_2013_buf_500<- st_buffer(fire_boundaries2013, 500)
fire_boundaries_2014_buf_500<- st_buffer(fire_boundaries2014, 500)
fire_boundaries_2015_buf_500<- st_buffer(fire_boundaries2015, 500)
fire_boundaries_2016_buf_500<- st_buffer(fire_boundaries2016, 500)
fire_boundaries_2017_buf_500<- st_buffer(fire_boundaries2017, 500)
fire_boundaries_2018_buf_500<- st_buffer(fire_boundaries2018, 500)
fire_boundaries_2019_buf_500<- st_buffer(fire_boundaries2019, 500)
fire_boundaries_2020_buf_500<- st_buffer(fire_boundaries2020, 500)

```

```{r}

st_write(fire_boundaries_2002_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2002.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2003_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2003.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2004_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2004.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2005_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2005.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2006_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2006.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2007_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2007.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2008_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2008.shp", delete_dsn = TRUE)


st_write(fire_boundaries_2009_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2009.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2010_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2010.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2011_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2011.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2012_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2012.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2013_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2013.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2014_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2014.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2015_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2015.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2016_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2016.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2017_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2017.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2018_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2018.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2019_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2019.shp", delete_dsn = TRUE)

st_write(fire_boundaries_2020_buf_500, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\Fire_boundaries_withbuffer_2020.shp", delete_dsn = TRUE)


```

Now that we have each year's polygons and buffers, we can go to QGIS and load each polygon-buffer set in QGIS and get the difference so that the buffer is separate from the fire boundary polygon. In QGIS, we will also load in the BC Boundary layer and then use the "Create Grid" function to create points that are separated by 100 m vertically and horizontally to represent 1 ha portions of the province. Because of the way this behaved, a second square for the SE corner of BC was created as well. We will then "clip" and "intersect" these points with the buffers and fire polygons for each year and designate the 0 and 1 values for whether fire spread into that cell (fire boundary) or not (500 or 150 m buffer, minus fire boundary). We can then combine all of the point data into one shape file, bring back into R, and append the topography and wind speed data. Note, however, that I (Cora) was having a hard time getting the final file to save in R, and thus the following steps were repeated in QGIS to get the shape file to save. Subsampling of the 5.5 million points may be necessary for work-flow downstream given that this is a large dataset. Once we have the relevant variables appended in either R or QGIS, we will then attach VRI data the same way we did for ignition data prep, using the command line. Once we have all of that data appended, we can assess how spread 0/1 is impacted by slope, elevation, aspect, wind speed, landuse type (bclcs_5). We will need to combine some of the bclcs_5 land types together (e.g., rock and bare ground types) since data size is small and it eats up degrees of freedom. We suggest also subsetting data for only >1ha.

############## If attempting to do the procedure in R, doin the following ######
Otherwise, skip to after the R processing and bring in from QGIS.

#Bring in the data. 
You can bring both in now, or you can work with one and then the other, or only with the 150 m buffer file.
```{r}
spread_150<-st_read(dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\fire_and_buffer_allyears_150m.shp", stringsAsFactors = T)
head(spread_150)

spread_500<- st_read(dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\As of August 2021\\PROT_HISTORICAL_FIRE_POLYS_SP\\fire_and_buffer_allyears_500m.shp", stringsAsFactors = T)
head(spread_500)
  
```

#Subset data
Because these files are so large, they will likely crash R when trying to proess them or save subsequent files. Thus, we will select only fires that are >1 ha (i.e., have escaped) for this dataset.

```{r}
spread_150_1ha<-subset(spread_150, spread_150$SIZE_HA>=1)
head(spread_150_1ha)
str(spread_150_1ha) #Reduces from 5,364,129 to 5,356,137 points... so still very large
spread_500_1ha<-subset(spread_500, spread_500$SIZE_HA>=1)
head(spread_500_1ha)
str(spread_500_1ha) #Reduced from 6,860,250 to 6,804,140... so lots still too
```

#Clear the data with less than 1 ha size fires
```{r}
library(gdata)
keep(spread_150_1ha, spread_500_1ha, sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)
```

#Load in the DEM layers and get aspect, slope and elevation

```{r}
##Slope
DEM_slope <- raster("T:\\FOR\\VIC\\HTS\\ANA\\PROJECTS\\CLUS\\Data\\dem\\all_bc\\slope_ha_bc.tif")

##Aspect
DEM_aspect <- raster("T:\\FOR\\VIC\\HTS\\ANA\\PROJECTS\\CLUS\\Data\\dem\\all_bc\\aspect_ha_bc.tif")

#Elevation
DEM <- raster("T:\\FOR\\VIC\\HTS\\ANA\\PROJECTS\\CLUS\\Data\\dem\\all_bc\\dem_ha_bc.tif")

```

Stack the rasters

```{r}
rasStack = stack(DEM_slope, DEM_aspect, DEM)
crs(rasStack)
head(rasStack)
str(rasStack)

```

Get the point coordinates for the point data

```{r}
#Extract coordinates for points
test_150<-cbind(spread_150_1ha, st_coordinates(spread_150_1ha))
head(test_150)

test_500<-cbind(spread_500_1ha, st_coordinates(spread_500_1ha))
head(test_500)

#Get as coordinates
pointCoordinates_150<-data.frame(test_150$X.1, test_150$Y.1)
head(pointCoordinates_150)

pointCoordinates_500<-data.frame(test_500$X.1, test_500$Y.1)
head(pointCoordinates_500)


##Extract DEM values from stacked layer
rasValue_150=raster::extract(rasStack, pointCoordinates_150)
head(rasValue_150)
str(rasValue_150) #5356127 points
str(spread_150) #5356127 points

rasValue_500=raster::extract(rasStack, pointCoordinates_500)
head(rasValue_500)
str(rasValue_500) #6804140
str(spread_500) #6804140

#Append new information
spread_150_DEM<-cbind(spread_150_1ha, rasValue_150)
head(spread_150_DEM)
crs(spread_150_DEM)

spread_500_DEM<-cbind(spread_500_1ha, rasValue_500)
head(spread_500_DEM)
crs(spread_500_DEM)

```

Save the DEM layer file. This save will take several hours since there is over 5,000,000 data points, so wait until further steps are complete if that is an option. Note: when I attempted to save these, it crashed after 10 hours. Avoid saving at this time. Will potentially need to break down into separate years to save, but this is not the saving error being encountered as of September 2021 (but may have been in August 2021).

```{r}
#st_write(spread_150_DEM, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM.shp", delete_layer=TRUE)

#st_write(spread_500_DEM, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_500_DEM.shp", delete_layer=TRUE)
```

#Clear the data again other than our working datasets
```{r}
library(gdata)
keep(spread_150_DEM, spread_500_DEM, sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)
```

#Now determine the road density for each point
Note: road density will be similar for each point, but slightly difference since they are 100 m apart. This could be a bit of an issue, however, given that each point is not necessarily independent. Road density and distance to habitation will likely be more appropriate for the fire size metric as opposed to the individual spread points. But we will include them in the data here in case it becomes clear that it is important. This aspect may or may not be appended within QGIS.

Bring in the roads layer.

```{r}
conn <- DBI::dbConnect (dbDriver ("PostgreSQL"), 
                        host = keyring::key_get('dbhost', keyring = 'postgreSQL'), 
                        dbname = keyring::key_get('dbname', keyring = 'postgreSQL'), 
                        port = '5432',
                        user = keyring::key_get('dbuser', keyring = 'postgreSQL'),
                        password = keyring::key_get('dbpass', keyring = 'postgreSQL'))

roads_layer<- st_read (dsn = conn, 
          layer = c ("public", "integrated_roads"))
dbDisconnect (conn)

```

Inspect the file. Check the coordinate system. Determine the names of items in the file.

```{r}
crs(roads_layer)
head(roads_layer)
names(roads_layer)

```

Now, we need to buffer each point from the sample point files by 0.564 km or 564 m to get a 1 km^2 area. Once we have this buffer, we can then perform an intersect to determine how many km of road we have per km^2 of area for each data point. This will involve summing over multiple rows because each road segment will create a new row within the dataframe but the same polygon id may appear multiple times.

```{r}
spread_150_DEM_buf<- st_buffer(spread_150_DEM, 564) #Set the buffer to 564 m to get a circle with a radius of 564 m and an area of 1 km2

crs(spread_150_DEM_buf)
head(spread_150_DEM_buf)

# ID number should be in both files
spread_150_DEM$id2<-c(1:5356127) #This number is the number of data points within the file
spread_150_DEM_buf$id2<-c(1:5356127)
```

Perform st_intersection between the roads layer and our buffered layer to get roads clipped to the area of the polygons. Because we have so many data points, the intersect function will not succeed on the full data set. Thus, we need to separate the data into chunks.

```{r}
#Start with 150 m buffer
spread_150_DEM_buf_2002_08<-subset(spread_150_DEM_buf, spread_150_DEM_buf$FIRE_YEAR<2009)
table(spread_150_DEM_buf_2002_08$FIRE_YEAR)

spread_150_DEM_buf_2009_20<-subset(spread_150_DEM_buf, spread_150_DEM_buf$FIRE_YEAR>2008)

spread_150_DEM_buf_2009_13<-subset(spread_150_DEM_buf_2009_20, spread_150_DEM_buf_2009_20$FIRE_YEAR<2014)

spread_150_DEM_buf_2017_20<-subset(spread_150_DEM_buf_2009_20, spread_150_DEM_buf_2009_20$FIRE_YEAR>2016)
table(spread_150_DEM_buf_2017_20$FIRE_YEAR)

spread_150_DEM_buf_2017<-subset(spread_150_DEM_buf,spread_150_DEM_buf$FIRE_YEAR==2017) #Will need to split this one in half or thirds
spread_150_DEM_buf_2018<-subset(spread_150_DEM_buf_2017_20,spread_150_DEM_buf_2017_20$FIRE_YEAR==2018) #Will need to split this one in half
spread_150_DEM_buf_2019_20<-subset(spread_150_DEM_buf_2017_20,spread_150_DEM_buf_2017_20$FIRE_YEAR>2018)

spread_150_DEM_buf_2009_16<-subset(spread_150_DEM_buf_2009_20, spread_150_DEM_buf_2009_20$FIRE_YEAR<2017)
spread_150_DEM_buf_2014_16<-subset(spread_150_DEM_buf_2009_16, spread_150_DEM_buf_2009_16$FIRE_YEAR>2013)

#Split 2017 and 2018 data in half for processing
prop<-0.5
DivideData <- createDataPartition(spread_150_DEM_buf_2017$spread, p = prop,
                                    list = FALSE,
                                    times = 1)
  
   spread_150_DEM_buf_2017_a <- spread_150_DEM_buf_2017[ DivideData,]
   spread_150_DEM_buf_2017_b <- spread_150_DEM_buf_2017[-DivideData,]
   
DivideData2 <- createDataPartition(spread_150_DEM_buf_2018$spread, p = prop,
                                    list = FALSE,
                                    times = 1)
  
   spread_150_DEM_buf_2018_a <- spread_150_DEM_buf_2018[ DivideData2,]
   spread_150_DEM_buf_2018_b <- spread_150_DEM_buf_2018[-DivideData2,]

#Now work with spread_150_DEM_buf_2002_08, spread_150_DEM_buf_2009_13, spread_150_DEM_buf_2014_16, spread_150_DEM_buf_2017_a, spread_150_DEM_buf_2017_b, spread_150_DEM_buf_2018_a, spread_150_DEM_buf_2018_b, spread_150_DEM_buf_2019_20

spread_150_roads_buffers_2002_08<-st_intersection(spread_150_DEM_buf_2002_08, roads_layer)
head(spread_150_roads_buffers_2002_08)

spread_150_roads_buffers_2009_13<-st_intersection(spread_150_DEM_buf_2009_13, roads_layer)
head(spread_150_roads_buffers_2009_13)

spread_150_roads_buffers_2014_16<-st_intersection(spread_150_DEM_buf_2014_16, roads_layer)
head(spread_150_roads_buffers_2014_16)

spread_150_roads_buffers_2017_a<-st_intersection(spread_150_DEM_buf_2017_a, roads_layer)
head(spread_150_roads_buffers_2017_a)

spread_150_roads_buffers_2017_b<-st_intersection(spread_150_DEM_buf_2017_b, roads_layer)
head(spread_150_roads_buffers_2017_b)

spread_150_roads_buffers_2018_a<-st_intersection(spread_150_DEM_buf_2018_a, roads_layer)
head(spread_150_roads_buffers_2018_a)

spread_150_roads_buffers_2018_b<-st_intersection(spread_150_DEM_buf_2018_b, roads_layer)
head(spread_150_roads_buffers_2018_b)

spread_150_roads_buffers_2019_20<-st_intersection(spread_150_DEM_buf_2019_20, roads_layer)
head(spread_150_roads_buffers_2019_20)

```

Below you can selectively clear some code as you do different steps above. You likely will not be able to keep everything in to get to the next steps given space limitations in R. Ensure you do not accidentally rid of spread_150_DEM, spread_500_DEM and any layers you are still working with.

```{r}
keep(spread_150_DEM, spread_500_DEM, road_sums_df_150_not2017_18, road_sums_2002_08_df_150, road_sums_2009_13_df_150, road_sums_2014_16_df_150, road_sums_2019_20_df_150, roads_layer, spread_150_DEM_buf_2017, spread_150_DEM_buf_2017_a, spread_150_DEM_buf_2017_b, spread_150_DEM_buf_2018, spread_150_DEM_buf_2018_a, spread_150_DEM_buf_2018_b, spread_150_DEM_buf, sure = TRUE) # setting sure to TRUE removes variables not in the list

keep(spread_150_DEM, spread_500_DEM, road_sums_2002_08_df_150, road_sums_2009_13_df_150, road_sums_2014_16_df_150, road_sums_2019_20_df_150, roads_layer, spread_150_DEM_buf_2017, spread_150_DEM_buf_2017_a, spread_150_DEM_buf_2017_b, spread_150_DEM_buf_2018, spread_150_DEM_buf_2018_a, spread_150_DEM_buf_2018_b, spread_150_DEM_buf, sure = TRUE) 

ls()
gc(TRUE)
```

Once we have the roads intersected with the polygon, we can then get the sums for each. Note, however, that if there are no roads for a location, that a 0 will be returned for those polygons (buffered circles) and that a special match will need to be performed to get the correct points appended back to the original data. Ensure you change the file to a data frame first so that it takes < 5 seconds to run instead of 4 hours.

```{r}
###########150 m buffer around fire perimeter
str(spread_150_roads_buffers)
spread_150_roads_buffers_2002_08_df<-as.data.frame(spread_150_roads_buffers_2002_08)
spread_150_roads_buffers_2009_13_df<-as.data.frame(spread_150_roads_buffers_2009_13)
spread_150_roads_buffers_2014_16_df<-as.data.frame(spread_150_roads_buffers_2014_16)
spread_150_roads_buffers_2017_a_df<-as.data.frame(spread_150_roads_buffers_2017_a)
spread_150_roads_buffers_2017_b_df<-as.data.frame(spread_150_roads_buffers_2017_b)
spread_150_roads_buffers_2017_df<-rbind(spread_150_roads_buffers_2017_a_df, spread_150_roads_buffers_2017_b_df)

spread_150_roads_buffers_2018_a_df<-as.data.frame(spread_150_roads_buffers_2018_a)
spread_150_roads_buffers_2018_b_df<-as.data.frame(spread_150_roads_buffers_2018_b)
spread_150_roads_buffers_2018_df<-rbind(spread_150_roads_buffers_2018_a_df, spread_150_roads_buffers_2018_b_df)

spread_150_roads_buffers_2019_20_df<-as.data.frame(spread_150_roads_buffers_2019_20)


#Summarize by id
road_sums_2002_08_df_150<- spread_150_roads_buffers_2002_08_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2009_13_df_150<- spread_150_roads_buffers_2009_13_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2014_16_df_150<- spread_150_roads_buffers_2014_16_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2019_20_df_150<- spread_150_roads_buffers_2019_20_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )
```


```{r}
##You may need to clear workspace here. Try running the code below this first before seeing if clearing is required.
keep(spread_150_DEM, spread_500_DEM, road_sums_2002_08_df_150, road_sums_2009_13_df_150, road_sums_2014_16_df_150, road_sums_2019_20_df_150, roads_layer, spread_150_DEM_buf_2017, spread_150_DEM_buf_2017_a, spread_150_DEM_buf_2017_b, spread_150_DEM_buf_2018, spread_150_DEM_buf_2018_a, spread_150_DEM_buf_2018_b, spread_150_roads_buffers_2018_df, spread_150_DEM_buf, spread_150_roads_buffers_2017_df, sure = TRUE)
```


```{r}
#Continue
road_sums_2017_df_150<- spread_150_roads_buffers_2017_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2018_df_150<- spread_150_roads_buffers_2018_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )


#Now combine the groups back together
road_sums_df_150<-rbind(road_sums_2002_08_df_150, road_sums_2009_13_df_150, road_sums_2014_16_df_150, road_sums_2017_df_150, road_sums_2018_df_150, road_sums_2019_20_df_150)

write.csv(road_sums_df_150, "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\road_sums_df_150_.csv")

table(road_sums_df_150$id)
hist(road_sums_df_150$shape_length)
head(road_sums_df_150)
```


```{r}
#Some clearing code if needed; do not run necessarily if not needed
keep(road_sums_df_150, spread_150_DEM, spread_500_DEM, spread_150_DEM_buf, spread_500_DEM_buf,  sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)

```

Now we will append this new data back to our original data. We also need to replace all of the NA values with 0s, as an NA indicates that there were no roads present in that polygon buffer.

```{r}
######### 150 m buffers
crs(road_sums_df_150) #Check if it has a coordinate system - no (as expected)
table(road_sums_df_150$id2)
str(road_sums_df_150$id2)

spread_150_DEM_roads<- merge(spread_150_DEM, road_sums_df_150, by = 'id2', all=TRUE) #Somehow this takes one file with 2.3 million points and one with 5.3 million points to create a file with 8.8 million points... this makes no sense.

crs(spread_150_DEM_roads) #has coordinate system as expected
head(spread_150_DEM_roads) #check if you can see any with NA in the 

str(spread_150_DEM_roads)

#Replace NAs in shape_length column with 0s
spread_150_DEM_roads$shape_length[is.na(spread_150_DEM_roads$shape_length)] <- 0
head(spread_150_DEM_roads) 
hist(spread_150_DEM_roads$shape_length)
max(spread_150_DEM_roads$shape_length) #99346.1
mean(spread_150_DEM_roads$shape_length) #2638.276

#Put into kms for better intuitive sense
spread_150_DEM_roads$roads_km<-spread_150_DEM_roads$shape_length/1000

hist(spread_150_DEM_roads$roads_km)
max(spread_150_DEM_roads$roads_km) #99.34
mean(spread_150_DEM_roads$roads_km) #2.63

#See year attributes
table(spread_150_DEM_roads$FIRE_YEAR) 

```

Save file. Note: many errors occurred for 2 weeks, and I was unable to figure out why the error was occurring.

```{r}
st_write(spread_150_DEM_roads, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads.shp", delete_layer=TRUE)

spread_150_DEM_roads_df<-data.frame((spread_150_DEM_roads))

write.csv(spread_150_DEM_roads_df, "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_df.csv")

```

Clear workspace and then repeat for 500 m. Potentially no need to include spread_150_DEM_roads_df because we can load it back in.

```{r}
keep(spread_150_DEM_roads, spread_150_DEM_roads_df, spread_500_DEM, spread_150_DEM, spread_150_DEM_buf, sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)
```

If you wish to inspect with a 500 m buffer around each fire perimeter instead of a 150 m buffer, then repeat the above but for 500 m buffer.

Create buffers.

```{r}
spread_500_DEM_buf<- st_buffer(spread_500_DEM, 564) #Set the buffer to 564 m to get a circle with a radius of 564 m and an area of 1 km2

crs(spread_500_DEM_buf)
head(spread_500_DEM_buf) 

spread_500_DEM$id2<-c(1:6804140)
spread_500_DEM_buf$id2<-c(1:6804140)
```

Separate buffer into several year chunks so that processing cab occur without crashing R.

```{r}
spread_500_DEM_buf_2002_08<-subset(spread_500_DEM_buf, spread_500_DEM_buf$FIRE_YEAR<2009)
table(spread_500_DEM_buf_2002_08$FIRE_YEAR)
head(spread_500_DEM_buf_2002_08)
spread_500_DEM_buf_2002_08$id2 


spread_500_DEM_buf_2009_20<-subset(spread_500_DEM_buf, spread_500_DEM_buf$FIRE_YEAR>2008)

spread_500_DEM_buf_2009_13<-subset(spread_500_DEM_buf_2009_20, spread_500_DEM_buf_2009_20$FIRE_YEAR<2014)

spread_500_DEM_buf_2017_20<-subset(spread_500_DEM_buf_2009_20, spread_500_DEM_buf_2009_20$FIRE_YEAR>2017)

spread_500_DEM_buf_2017_18<-subset(spread_500_DEM_buf_2017_20,spread_500_DEM_buf_2017_20$FIRE_YEAR<2019)
spread_500_DEM_buf_2019_20<-subset(spread_500_DEM_buf_2017_20,spread_500_DEM_buf_2017_20$FIRE_YEAR>2018)

spread_500_DEM_buf_2009_16<-subset(spread_500_DEM_buf_2009_20, spread_500_DEM_buf_2009_20$FIRE_YEAR<2017)
spread_500_DEM_buf_2014_16<-subset(spread_500_DEM_buf_2009_16, spread_500_DEM_buf_2009_16$FIRE_YEAR>2013)


#Now work with spread_500_DEM_buf_2002_08, spread_500_DEM_buf_2009_13, spread_500_DEM_buf_2014_16, spread_500_DEM_buf_2017_18, spread_500_DEM_buf_2019_20

spread_500_roads_buffers_2002_08<-st_intersection(spread_500_DEM_buf_2002_08, roads_layer)
head(spread_500_roads_buffers_2002_08)

spread_500_roads_buffers_2009_13<-st_intersection(spread_500_DEM_buf_2009_13, roads_layer)
head(spread_500_roads_buffers_2009_13)

spread_500_roads_buffers_2014_16<-st_intersection(spread_500_DEM_buf_2014_16, roads_layer)
head(spread_500_roads_buffers_2014_16)

spread_500_roads_buffers_2017_20<-st_intersection(spread_500_DEM_buf_2017_20, roads_layer)
head(spread_500_roads_buffers_2017_20)
```

Change to a dataframe and then summarize by id2 (the one we created).

```{r}
###########500 m buffer around fire perimeter
str(spread_500_roads_buffers)
spread_500_roads_buffers_2002_08_df<-as.data.frame(spread_500_roads_buffers_2002_08)
spread_500_roads_buffers_2009_13_df<-as.data.frame(spread_500_roads_buffers_2009_13)
spread_500_roads_buffers_2014_16_df<-as.data.frame(spread_500_roads_buffers_2014_16)
spread_500_roads_buffers_2017_20_df<-as.data.frame(spread_500_roads_buffers_2017_20)


#Summarize by id
road_sums_2002_08_df_500<- spread_500_roads_buffers_2002_08_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2009_13_df_500<- spread_500_roads_buffers_2009_13_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2014_16_df_500<- spread_500_roads_buffers_2014_16_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

road_sums_2017_20_df_500<- spread_500_roads_buffers_2017_20_df %>%
  group_by(id2) %>% #Check what ID is needed!! May be OBJECTID? Needs to be different for each point.
  summarize(
    shape_length = sum(shape_length)
  )

#Now combine the groups back together
road_sums_df_500<-rbind(road_sums_2002_08_df_500, road_sums_2009_13_df_500, road_sums_2014_16_df_500, road_sums_2017_20_df_500)

table(road_sums_df_500$id2)
head(road_sums_df_500)
str(road_sums_df_500)#Check how many rows there are in the data and compare to our initial data
#181,356
str(spread_500_roads_buffers) # 274594 obs
#Note that there is a difference because not all cells had roads!

```

Now we will append this new data back to our original spread_locations_DEM data. We also need to replace all of the NA values with 0s, as an NA indicates that there were no roads present in that polygon buffer.

```{r}
###500 m buffers
crs(road_sums_df_500) #Check if it has a coordinate system - no (as expected)

spread_500_DEM_roads<- merge(spread_500_roads_buffers, road_sums_df_500, by = 'id2', all=TRUE)
crs(spread_500_DEM_roads) #has coordinate system as expected
head(spread_500_DEM_roads) #check if you can see any with NA in the 

str(spread_500_DEM_roads)
#Check if need this below?
#spread_500_DEM_roads_sf<-st_as_sf(spread_500_DEM_roads)

#Replace NAs in shape_length column with 0s
spread_500_DEM_roads$shape_length[is.na(spread_500_DEM_roads$shape_length)] <- 0
head(spread_500_DEM_roads) #We can see that NAs have not been switched to 0

```

Inspect the results to ensure nothing seems too crazy

```{r}

##500 m buffer
hist(spread_500_DEM_roads$shape_length)
mean(spread_500_DEM_roads$shape_length) 
max(spread_500_DEM_roads$shape_length) 

##Convert to kms worth of road
spread_500_DEM_roads$shape_length_km<-(spread_500_DEM_roads$shape_length/1000)

#Inspect: note, below has not been updated with notes since id2 was created to fix issue
hist(spread_500_DEM_roads$shape_length_km, n=100)
mean(spread_500_DEM_roads$shape_length_km) #Mean is 3.8 km of road per km square
max(spread_500_DEM_roads$shape_length_km) #max is 207 km of road per km square... seems unlikely!

spread_500_DEM_roads_2<-subset(spread_500_DEM_roads, spread_500_DEM_roads$shape_length_km < 100)
hist(spread_500_DEM_roads_2$shape_length_km, n=100) #Is outlier removed? New max is 138 if subset at the 200 km level.
max(spread_500_DEM_roads_2$shape_length_km)
str(spread_500_DEM_roads_2) #274573 observations -- only lose 21 observations and likely more accurate
str(spread_500_DEM_roads) #274594 observations

```

Save file

```{r}

st_write(spread_500_DEM_roads_2, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_500_DEM_roads.shp", delete_layer=TRUE)
```

# Add Wind Speed
we will also add wind speed, but this likely will also not be relevant for the spread data, but instead for the fire size portion of spread. It may be useful to append here if we want more specific values, such as means over the entire polygon as opposed to the point location indicated for the initial fire.

Load shape files for both the created summer file, and for BC Boundaries.

```{r}
summer_wind_raster<- raster("D:\\Fire\\fire_data\\raw_data\\GovCanadaWindFiles\\wind_summer_clipped_224.tif")
res(summer_wind_raster)

spring_wind_raster<- raster("D:\\Fire\\fire_data\\raw_data\\GovCanadaWindFiles\\wind_spring_raster_224.tif")
res(spring_wind_raster) 

```

Stack the rasters.

```{r}
rasStackWind = stack(summer_wind_raster, spring_wind_raster)
crs(rasStackWind)
head(rasStackWind)
str(rasStackWind)
```

Extract Wind Values.

```{r}
##Extract Coordinates
test<-cbind(spread_150_DEM_roads, st_coordinates(spread_150_DEM_roads))
head(test)

pointCoordinates<-data.frame(test$X.1, test$Y.1)
head(pointCoordinates)
#crs(pointCoordinates) #No CRS when a dataframe

##Extract Wind values from stacked layer
rasValue3=raster::extract(rasStackWind, pointCoordinates)
head(rasValue3)
str(rasValue3) # 5356127 values
str(spread_150_DEM_roads)# 5356127 values

#Append new information
spread_150_DEM_roads_wind<-cbind(spread_150_DEM_roads, rasValue3)
head(spread_150_DEM_roads_wind)
crs(spread_150_DEM_roads_wind)
```

Create more space in R.

```{r}
keep(spread_150_DEM_roads_wind, spread_500_DEM, sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)
```
Save data. Note: several errors in saving, could not figure out why. There is a lot of trouble shooting code below for consideration, but none were able to resolve the problem. The same problems existed when I subset the data to a single year (e.g., 2002), so file size is *likely* not the main issue.

```{r}
str(spread_150_DEM_roads_wind)
#spread_150_DEM_roads_wind_sf<-st_as_sf(spread_150_DEM_roads_wind)
#head(spread_150_DEM_roads_wind_sf)
#str(spread_150_DEM_roads_wind_sf)

#Because there are a lot of unneeded columns with long paths, remove those and select only the columns of interest.
spread_150_DEM_roads_wind_2<-select(spread_150_DEM_roads_wind, 'id2','FIRE_NO', 'FIRE_YEAR', 'FIRE_CAUSE','SIZE_HA', 'FIRE_DATE', 'OBJECTID', 'spread', 'id', 'slope_ha_bc', 'aspect_ha_bc', 'dem_ha_bc', 'roads_km', 'wind_summer_clipped_224', 'wind_spring_raster_224', 'geometry')


str(spread_150_DEM_roads_wind_2)

#Rename columns
spread_150_DEM_roads_wind_2<-spread_150_DEM_roads_wind_2 %>% rename(win_sum=wind_summer_clipped_224,
      win_spr=wind_spring_raster_224,
      aspect = aspect_ha_bc,
      slope = slope_ha_bc,
      elev = dem_ha_bc,
      fire_cs=FIRE_CAUSE)


sf::st_write(spread_150_DEM_roads_wind_2, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_wind.shp", delete_layer=TRUE)


#Save as dataframe as well
spread_150_DEM_roads_wind_2_df<-data.frame((spread_150_DEM_roads_wind_2))

write.csv(spread_150_DEM_roads_wind_2_df, "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_wind_df.csv")

```

If you can get the above to work up to this point, the next step will be to bring the data into QGIS with each of the infrastructure layers to determine the distance to each point. Note: with 5.5 million points, this is a LOT of points to determine distance to. This will be a very long procedure, and sub-sampling the data prior to this point may be advisable.

We may also wish to subsample the burn areas to reduce the number of data points.

```{r}
spread_150_DEM_roads_wind_NOFIRE<-subset(spread_150_DEM_roads_wind_2, spread_150_DEM_roads_wind_2$spread==0)

spread_150_DEM_roads_wind_FIRE<-subset(spread_150_DEM_roads_wind_2, spread_150_DEM_roads_wind_2$spread==1)
head(spread_150_DEM_roads_wind_FIRE)

#Take half of the fire datapoints, separated by OBJECTID so that we take half of the points from each fire.
prop<-0.5
DivideData2 <- createDataPartition(spread_150_DEM_roads_wind_FIRE$OBJECTID, p = prop,
                                    list = FALSE,
                                    times = 1)
  

spread_150_DEM_roads_wind_FIRE_halfpoints <- spread_150_DEM_roads_wind_FIRE[ DivideData2,]

#Combine no fire points with half fire points
spread_150_DEM_roads_wind_half<-rbind(spread_150_DEM_roads_wind_NOFIRE, spread_150_DEM_roads_wind_FIRE_halfpoints)
```


```{r}
#Repeat for 20% of points
prop<-0.2
DivideData3 <- createDataPartition(spread_150_DEM_roads_wind_FIRE$OBJECTID, p = prop,
                                    list = FALSE,
                                    times = 1)
  

spread_150_DEM_roads_wind_FIRE_20percent <- spread_150_DEM_roads_wind_FIRE[ DivideData3,]

#Combine no fire points with half fire points
spread_150_DEM_roads_wind_20per<-rbind(spread_150_DEM_roads_wind_NOFIRE, spread_150_DEM_roads_wind_FIRE_20percent)

```


```{r}
#Repeat for 10% of points
prop<-0.1
DivideData3 <- createDataPartition(spread_150_DEM_roads_wind_FIRE$OBJECTID, p = prop,
                                    list = FALSE,
                                    times = 1)
  

spread_150_DEM_roads_wind_FIRE_10percent <- spread_150_DEM_roads_wind_FIRE[ DivideData3,]

#Combine no fire points with half fire points
spread_150_DEM_roads_wind_10per<-rbind(spread_150_DEM_roads_wind_NOFIRE, spread_150_DEM_roads_wind_FIRE_10percent)

```

You may need to remove field "AREA_SQM" and "id" because it has >7 numbers for some fires and this causes many warning messages that tends to crash R over >1 million points.

```{r}
spread_150_DEM_roads_wind_FIRE_10percent<-select(spread_150_DEM_roads_wind, 'id2','FIRE_NO', 'FIRE_YEAR', 'FIRE_CAUSE','SIZE_HA', 'FIRE_DATE', 'OBJECTID', 'AREA_SQM', 'spread', 'slope_ha_bc', 'aspect_ha_bc', 'dem_ha_bc', 'roads_km', 'wind_summer_clipped_224', 'wind_spring_raster_224', 'geometry')
```

Attempt to save files.

```{r}
sf::st_write(spread_150_DEM_roads_wind_10per, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_wind_10per.shp", delete_layer=TRUE)

sf::st_write(spread_150_DEM_roads_wind_20per, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_wind_20per.shp", delete_layer=TRUE)

sf::st_write(spread_150_DEM_roads_wind_half, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\spread_150_DEM_roads_wind_50per.shp", delete_layer=TRUE)

```

Clear the work space, keeping what may be needed.

```{r}
keep(spread_150_DEM_roads_wind_half, spread_150_DEM_roads_wind_10per, spread_150_DEM_roads_wind_20per, spread_150_DEM_roads_wind_2, sure = TRUE) # setting sure to TRUE removes variables not in the list

ls()
gc(TRUE)
```

##Performing instead in QGIS
The above can be very difficult to save in R, causing many crashes and lack of success. You may wish to repeat the above process in QGIS. Process: First,I brought in the wind rasters created previously (spring and summer), and then I brought in the DEM layers (slope, aspect and elevation). I used the "Sample Raster Values" function in QGIS to get the values for each of these 5 variables for each point. I then brought in each of the infrastructure layers (dams, natural resource infrastructure, railways, transmission lines, municipalities, mines) to determine the distance to each object, using the NNJoin extension. 

## Now bring data back in
Regardless of whether the R code above worked for DEM, wind and roads, or if those procedures were performed in QGIS, the subsequent step would be to bring the layer into QGIS to use the NNJoin function to determine the distance to each infrastructure type. Below, we bring this in and manage the data, prior to appending the VRI data.

Note: TBD if this file will be able to be saved after processing. May need to do this step in QGIS as well. If do in R, use below code. Otherwise, in QGIS, use the Field Calculator, and the min(A,B,C...) function.

#load in the data where the distances to infrastrcture for each location is already determined. This will be the file for if the minimum distance is not determined prior.
```{r}
spread_locations_distances<-st_read(dsn = "D:\\Fire\\fire_data\\raw_data\\Infrastructure and Urban Areas\\Distance_To_Points.shp") ### This needs to be updated!
head(spread_locations_distances)
```

#Update column names
You will need to update the column names regardless of whether the minimum distance was determined in QGIS or in R below. You can change the names of the columns here if brought back ino R, or you can change the names in QGIS.
```{r}
spread_locations_distances$dist_mun<-spread_locations_distances$distance
spread_locations_distances$dist_dam <-spread_locations_distances$distance_d
spread_locations_distances$dist_nat <-spread_locations_distances$distance_N
spread_locations_distances$dist_rail <-spread_locations_distances$distance_R
spread_locations_distances$dist_pow <-spread_locations_distances$distance_T
spread_locations_distances$dist_mine <-spread_locations_distances$distance_M
```

Create a column that is the shortest distance to any infrastructure.

```{r}
head(spread_locations_distances)
str(spread_locations_distances)

str(spread_locations_distances$dist_mine)
str(spread_locations_distances$dist_pow)
str(spread_locations_distances$dist_rail) #numeric but all NAs
str(spread_locations_distances$dist_nat)
str(spread_locations_distances$dist_dam)
str(spread_locations_distances$dist_mun)

names(spread_locations_distances)

spread_locations_distances$dist_any_<-0
spread_locations_distances$dist_any<-0

spread_locations_distances$dist_any_<-
  ifelse(spread_locations_distances$dist_mine<spread_locations_distances$dist_pow, spread_locations_distances$dist_mine, spread_locations_distances$dist_pow)
str(spread_locations_distances$dist_any_)

spread_locations_distances$dist_any<-
  ifelse (spread_locations_distances$dist_any_<spread_locations_distances$dist_nat, spread_locations_distances$dist_any_, spread_locations_distances$dist_nat)
str(spread_locations_distances$dist_any)

spread_locations_distances$dist_any_<-
  ifelse (spread_locations_distances$dist_any<spread_locations_distances$dist_dam, spread_locations_distances$dist_any, spread_locations_distances$dist_dam)
str(spread_locations_distances$dist_any_)

spread_locations_distances$dist_any<-
  ifelse (spread_locations_distances$dist_any_<spread_locations_distances$dist_mun, spread_locations_distances$dist_any_, spread_locations_distances$dist_mun)
str(spread_locations_distances$dist_any)

#Distance rail
spread_locations_distances$dist_any<-
  ifelse (spread_locations_distances$dist_any_<spread_locations_distances$dist_rail,
spread_locations_distances$dist_any_, spread_locations_distances$dist_rail)

```

Save the file. Hopefully this works. Because of the risks, I have opted to process and save in QGIS to then save to postgres and append VRI data in postgres next without needing to save one of these large files in R.

```{r}
sf::st_write(spread_locations_distances, dsn = "D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\Data.spread.shp", delete_layer=TRUE)
```

##Append BEC data
Because we did not get ClimateBC, we are missing the BEC and Natural Disturbance Type data that our previous data has (for ignition and escape). Therefore, we must append the BEC data to the points. This can be done either with QGIS using the intersect tool, or in R. 

#Save completed file in Postgre. 

```{r}
#Save to postgre
#ogr2ogr -f PostgreSQL PG:"host=DC052586 user= dbname=clus password= port=5432" D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\Spread_DEM_roads_wind_20p_BEC_infraALL2.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

#ogr2ogr -f "PostgreSQL" PG:"host=localhost user=postgres dbname=postgres password=postgres port=5432" D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\Spread_DEM_roads_wind_20p_BEC_infraALL2.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

#Reduce name and remove capitals and try again
#Save to postgre
#ogr2ogr -f PostgreSQL PG:"host=DC052586 user= dbname=clus password= port=5432" D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\spread_20p_all.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

#ogr2ogr -f "PostgreSQL" PG:"host=localhost user=postgres dbname=postgres password=postgres port=5432" D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\spread_20p_all.shp -overwrite -a_srs EPSG:3005 -progress --config PG_USE_COPY YES -nlt PROMOTE_TO_MULTI

#key_get('dbpass', keyring = 'postgreSQL')
```


###### Append VRI data to above data ############
We will use the previously uploaded VRI files from 2002-2020.

#### Join ignition data to VRI data ####
Run following query in postgres for all years 2002-2019 except 2007 and 2008. 2020 also has slightly different code. This will need to be done each time new data is generated for random points in file. This below code is fast. Note, there are 5 locations in the below code for which to update the year.

Depending on how the file names most recently saved, some changes may need to be made (e.g., ign_mnt versus ign_month).

CREATE TABLE fire_spread_veg_2002 AS
(SELECT feature_id, bclcs_level_2, bclcs_level_3, bclcs_level_4, bclcs_level_5,
  harvest_date, proj_age_1, proj_height_1, live_stand_volume_125, 
  fire.id2, fire.fire_year, fire.fire_cs, fire.size_ha, fire.fire_date,
  fire.objectid, fire.spread, fire.id, fire.slope, fire.aspect, fire.elev,
  fire.roads_km, fire.win_sum, fire.win_spr, fire.zone, fire.subzone, 
  fire.variant, fire.ntrldstrbn, fire.distance_m, fire.distance,
  fire.dist_nat, fire.distance_p, fire.dist_mines, fire.dist_any,
  veg_comp_lyr_r1_poly2002.geometry FROM veg_comp_lyr_r1_poly2002,
  (SELECT wkb_geometry, id2, FIRE_YEAR, fire_cs, size_ha, fire_date, objectid, 
  spread, id, slope, aspect, elev, roads_km, win_sum, win_spr, zone, subzone, 
  variant, ntrldstrbn, distance_m, distance, dist_nat, distance_p, dist_mines, dist_any 
   from spread_20p_all where FIRE_YEAR = '2002') as fire where st_contains
  (veg_comp_lyr_r1_poly2002.geometry, fire.wkb_geometry))


## See specifics of land cover types here: https://www2.gov.bc.ca/assets/gov/environment/natural-resource-stewardship/nr-laws-policy/risc/landcover-02.pdf
bclcs_level_2: The second level of the BC land cover classification scheme classifies the polygon as to the land cover type:
   # treed or non-treed for vegetated polygons; land or water for non-vegetated polygons.
bclcs_level_3: The location of the polygon relative to elevation and drainage, and is described as either alpine, wetland
   # or upland. In rare cases, the polygon may be alpine wetland.
bclcs_level_4: Classifies the vegetation types and non-vegetated cover types (as described by the presence of distinct features
    # upon the land base within the polygon).
bclcs_level_5: Classifies the vegetation density classes and Non-Vegetated categories.


############## 
#For 2020, run this code because no projected age, height or stand volume 
##############

CREATE TABLE fire_spread_veg_2020 AS
(SELECT feature_id, bclcs_level_2, bclcs_level_3, bclcs_level_4, bclcs_level_5,
  harvest_date, 
  fire.id2, fire.fire_year, fire.fire_cs, fire.size_ha, fire.fire_date,
  fire.objectid, fire.spread, fire.id, fire.slope, fire.aspect, fire.elev,
  fire.roads_km, fire.win_sum, fire.win_spr, fire.zone, fire.subzone, 
  fire.variant, fire.ntrldstrbn, fire.distance_m, fire.distance,
  fire.dist_nat, fire.distance_p, fire.dist_mines, fire.dist_any,
  veg_comp_lyr_r1_poly2020.geometry FROM veg_comp_lyr_r1_poly2020,
  (SELECT wkb_geometry, id2, fire_year, fire_cs, size_ha, fire_date, objectid, 
  spread, id, slope, aspect, elev, roads_km, win_sum, win_spr, zone, subzone, 
  variant, ntrldstrbn, distance_m, distance, dist_nat, distance_p, dist_mines, dist_any 
   from spread_20p_all where FIRE_YEAR = '2020') as fire where st_contains
  (veg_comp_lyr_r1_poly2020.geometry, fire.wkb_geometry))
  
  
###################
#FOR 2007 run this because some of the names for the VRI_2007 file are different to what they are in other years. In particular:
###################

# bclcs_level_2 = bclcs_lv_2 same for the other bclcs layers
#proj_height_1 = proj_ht_1
#harvest_date = upd_htdate
# live_stand_volume_125 does not exist, so fancy stuff created
# percent_dead does not exist

 CREATE TABLE fire_spread_veg_2007 AS
 (SELECT feature_id, bclcs_lv_2, bclcs_lv_3, bclcs_lv_4, bclcs_lv_5,
  upd_htdate, proj_age_1, proj_ht_1, COALESCE(volsp1_125,0)+COALESCE(volsp2_125,0)+COALESCE(volsp3_125,0)+COALESCE(volsp4_125,0)+COALESCE(volsp5_125,0) AS live_stand_volume_125, 
  fire.id2, fire.fire_year, fire.fire_cs, fire.size_ha, fire.fire_date,
  fire.objectid, fire.spread, fire.id, fire.slope, fire.aspect, fire.elev,
  fire.roads_km, fire.win_sum, fire.win_spr, fire.zone, fire.subzone, 
  fire.variant, fire.ntrldstrbn, fire.distance_m, fire.distance,
  fire.dist_nat, fire.distance_p, fire.dist_mines, fire.dist_any,
  veg_comp_lyr_r1_poly2007.geometry FROM veg_comp_lyr_r1_poly2007,
  (SELECT wkb_geometry, id2, fire_year, fire_cs, size_ha, fire_date, objectid, 
  spread, id, slope, aspect, elev, roads_km, win_sum, win_spr, zone, subzone, 
  variant, ntrldstrbn, distance_m, distance, dist_nat, distance_p, dist_mines, dist_any 
   from spread_20p_all where fire_year = '2007') 
   as fire where st_contains (veg_comp_lyr_r1_poly2007.geometry, fire.wkb_geometry))


##############################
# FOR 2008 Run this code:
No  stand_percentage_dead, 
##############################
 CREATE TABLE fire_spread_veg_2008 AS
 (SELECT feature_id, bclcs_level_2, bclcs_level_3, bclcs_level_4, bclcs_level_5,
   harvest_date, proj_age_1, proj_height_1, COALESCE(vol_per_ha_spp1_125,0)+
     COALESCE(vol_per_ha_spp2_125,0)+COALESCE(vol_per_ha_spp3_125,0)+COALESCE(vol_per_ha_spp4_125,0)
   +COALESCE(vol_per_ha_spp5_125,0)+COALESCE(vol_per_ha_spp6_125,0) AS live_stand_volume_125,
   fire.id2, fire.fire_year, fire.fire_cs, fire.size_ha, fire.fire_date,
  fire.objectid, fire.spread, fire.id, fire.slope, fire.aspect, fire.elev,
  fire.roads_km, fire.win_sum, fire.win_spr, fire.zone, fire.subzone, 
  fire.variant, fire.ntrldstrbn, fire.distance_m, fire.distance,
  fire.dist_nat, fire.distance_p, fire.dist_mines, fire.dist_any,
   veg_comp_lyr_r1_poly2008.geometry FROM veg_comp_lyr_r1_poly2008,
   (SELECT wkb_geometry, id2, fire_year, fire_cs, size_ha, fire_date, objectid, 
  spread, id, slope, aspect, elev, roads_km, win_sum, win_spr, zone, subzone, 
  variant, ntrldstrbn, distance_m, distance, dist_nat, distance_p, dist_mines, dist_any 
    from spread_20p_all where fire_year = '2008') 
    as fire where st_contains (veg_comp_lyr_r1_poly2008.geometry, fire.wkb_geometry))


# Another problem is that fire_veg_2011 has a geometry column that is type MultiPolygonZ instead of MultiPolygon so this needs to be changed with the following query in postgres
 ALTER TABLE fire_spread_veg_2011  
 ALTER COLUMN geometry TYPE geometry(MULTIPOLYGON, 3005) 
 USING ST_Force2D(geometry);
 
 Now, we can use R for the next step.

=====================

```{r}
#Load necessary libraries

library(dplyr)
library(tidyr)
library(keyring)
library(sf)
library(DBI)
library(purrr)
library(tidyverse)
library(ggplot2)
library (ggcorrplot)
library (RPostgreSQL)
library (rpostgis)
library (lme4)
library (arm)
library(ggpubr)
library(mgcv)
library(nlme)
library(caret)
library(pROC)

source(here::here("R/functions/R_Postgres.R"))

```

Now we will bring all of the files we made above into R. You may need to create a new connection to bring each one in instead, or at least run line by line and not as a code chunk.

```{r}
#Import all fire_spread_veg
conn <- dbConnect (dbDriver ("PostgreSQL"), 
                   host = "",
                   user = "postgres",
                   dbname = "postgres",
                   password = "postgres",
                   port = "5432")
fire_spread_veg_2002 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2002")
fire_spread_veg_2003 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2003")
fire_spread_veg_2004 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2004")
fire_spread_veg_2005 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2005")
fire_spread_veg_2006 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2006")
fire_spread_veg_2007 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2007")
fire_spread_veg_2008 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2008")
fire_spread_veg_2009 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2009")
fire_spread_veg_2010 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2010")
fire_spread_veg_2011 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2011")
fire_spread_veg_2012 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2012")
fire_spread_veg_2013 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2013")
fire_spread_veg_2014 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2014")
fire_spread_veg_2015 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2015")
fire_spread_veg_2016 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2016")
fire_spread_veg_2017 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2017")
fire_spread_veg_2018 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2018")
fire_spread_veg_2019 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2019")
fire_spread_veg_2020 <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.fire_spread_veg_2020")

dbDisconnect (conn) # connKyle

```

We need to update naming conventions from the 2007 and 2008 data given that it is different in those VRI files than for other years.

```{r}
# REMEMBER TO CHANGE THE 0 values in the 2007 and 2008 fire_spread_veg datasets for volume to NULL.

# the VRI for 2007 had harvest_date named upd_htdate and proj_height_1, proj_height_2 as proj_ht_1, proj_ht_2. So I need to be renamed these columns
#fire_spread_veg_2007$harvest_date <- NA
names(fire_spread_veg_2007)
names(fire_spread_veg_2002)
fire_spread_veg_2007<- fire_spread_veg_2007 %>% rename(
  bclcs_level_2=bclcs_lv_2,
  bclcs_level_3=bclcs_lv_3,
  bclcs_level_4=bclcs_lv_4,
  bclcs_level_5=bclcs_lv_5,
  proj_height_1=proj_ht_1, 
  harvest_date=upd_htdate)


# change the 0 values in live_stand_volume_125 to NULL for both 2007 and 2008 data

fire_spread_veg_2007$live_stand_volume_125[fire_spread_veg_2007$live_stand_volume_125 == 0] <- NA
fire_spread_veg_2008$live_stand_volume_125[fire_spread_veg_2008$live_stand_volume_125 == 0] <- NA

fire_spread_veg_2020$proj_height_1<-NA
fire_spread_veg_2020$proj_age_1<-NA
fire_spread_veg_2020$live_stand_volume_125<-NA
fire_spread_veg_2020$stand_percentage_dead<-NA

```

2020 has 35 columns when the rest have 34; inspect.

```{r}
names(fire_spread_veg_2012)
names(fire_spread_veg_2020)
fire_spread_veg_2020<-fire_spread_veg_2020[-35]
names(fire_spread_veg_2020)
```


Now we must combine all of the files together into one.

```{r}
# join all fire_spread_veg datasets together. This function is faster than a list of rbinds
filenames3<- c("fire_spread_veg_2002", "fire_spread_veg_2003", "fire_spread_veg_2004","fire_spread_veg_2005", "fire_spread_veg_2006", "fire_spread_veg_2007","fire_spread_veg_2008", "fire_spread_veg_2009", "fire_spread_veg_2010","fire_spread_veg_2011", "fire_spread_veg_2012", "fire_spread_veg_2013","fire_spread_veg_2014", "fire_spread_veg_2015", "fire_spread_veg_2016","fire_spread_veg_2017", "fire_spread_veg_2018", "fire_spread_veg_2019", "fire_spread_veg_2020")
filenames3

mkFrameList <- function(nfiles) {
  d <- lapply(seq_len(nfiles),function(i) {
    eval(parse(text=filenames3[i])) # for new files lists change the name at filenames2
  })
  do.call(rbind,d)
}

n<-length(filenames3)
fire_spread_veg_data_B<-mkFrameList(n)

table(fire_spread_veg_data_B$fire_year, fire_spread_veg_data_B$fire_cs)

names(fire_spread_veg_data_B)

#Rename columns for ease of use
head(fire_spread_veg_data_B)
fire_spread_veg_data_B<- fire_spread_veg_data_B %>% rename(
  dist_mun=distance_m,
  dist_dam=distance,
  dist_mine=dist_mines,
  dist_pow=distance_p)
names(fire_spread_veg_data_B)
str(fire_spread_veg_data_B)

```

Clear code.

```{r}
library(gdata)

gdata::keep(fire_spread_veg_data_B, spread_data_lightning, spread_data_lightning_nt, spread_data_lightning_t, sure = TRUE)

```

Create a dataframe version.

```{r}
fire_spread_veg_data_df<-as.data.frame(fire_spread_veg_data_B)
str(fire_spread_veg_data_df)

fire_spread_veg_data_df<-st_drop_geometry(fire_spread_veg_data_B)
str(fire_spread_veg_data_df)
head(fire_spread_veg_data_df)

```

#Must create new columns for vegtype and bclcs_level_5_2 like in previous analyses

```{r}

#Repeat vegtype classification with new information
fire_spread_veg_data_df$vegtype2<-"OP" #setting anything that is not one of the categories below to Open.
fire_spread_veg_data_df <- fire_spread_veg_data_df %>%
  mutate(vegtype2 = if_else(bclcs_level_4=="TC","TC", # Treed coniferous
                           if_else(bclcs_level_4=="TM", "TM", # Treed mixed
                                   if_else(bclcs_level_4== "TB","TB", #Treed broadleaf
                                           if_else(bclcs_level_4=="SL", "S", # shrub
                                                   if_else(bclcs_level_4=="ST", "S", 
                                                           if_else(bclcs_level_4=="RO","RO",
                                                                   if_else(bclcs_level_4=="SI", "SI",                                         vegtype2))))))))
fire_spread_veg_data_df$vegtype2[which(fire_spread_veg_data_df$proj_age_1 <16)]<-"D" #
table(fire_spread_veg_data_df$vegtype2) #D: Disturbed; OP: Open; RO: Rock; S: Shrub; SI: Snow/Ice; TB: Treed Broadleaf; TC: Treed Conifer; TM: Treed Mixed.

#Combine Rock, ice and snow because ice and snow has too few points
fire_spread_veg_data_df$vegtype2[fire_spread_veg_data_df$vegtype2=="SI"] <- "RO"
table(fire_spread_veg_data_df$vegtype2)
```

Assess landuse type distribution
```{r}
table(fire_spread_veg_data_df$bclcs_level_5)
table(fire_spread_veg_data_df$bclcs_level_5, fire_spread_veg_data_df$fire)

```
AP: airport
BI: Block field (Blocks of rock derived from the underlying bedrock by weathering and / or frost heaving. These have not undergone and significant down slope movement as they occur on level or gently sloping areas. Was part of the undifferentiated Rubble, Talus, Block field in previous versions)
BP: unspecified -> seems to be old code for urban. Combine with urban.
BR: bedrock
BU: Burned Area
CB: cutbank (Part of a road corridor created upslope of the road surface, created by excavation into the hillside)
CL: closed (Cover of bryoids is greater than 50% of the polygon) --> this one confuses me... closed canopy? Closed what?
DE: dense (Tree, shrub, or herb cover is between 61% and 100% for the polygon)
ES: Exposed Soil (Any exposed soil not covered by the other categories, such as areas of recent disturbance that include mud slides, debris torrents, avalanches, or disturbances such as pipeline rights-of-way or cultivated fields where vegetation cover is less than 5%)
GP: Gravel Pit
LA: Lake
LL: Landing (A compacted area adjacent to a road used for sorting and loading logs)
LS: Pond or lake sediments (Exposed sediments related to dried lakes or ponds)
MI: Open Pit Mine
MN: Moraine (debris)
MU: Mudflat (fine-textured sediments)
MZ: Rubbly Mine Spoils (Discarded overburden or waste rock, moved to extract ore during mining.)
OC: Ocean
OP: Open (Cover of bryoids is less than or equal to 50% of the polygon)
OT: Other (A Non-Vegetated polygon where none of the above categories can be reliably chosen)
PN: Snow Cover (Snow or ice that is not part of a glacier but is found during summer months on the landscape)
RE: Reservoir
RI: River/Stream
RM: Reservoir margin
RN: Railway Surface
RS: River Sediments
RZ:  Road Surface
SP: sparse (Cover is between 10% and 25% for treed polygons, or cover is between 20% and 25% for shrub or herb polygons)
TA: Talus (Rock fragments of any size accumulated on or at the foot of slopes as a result of successive rock falls. This is a type of colluvium)
TS: unspecified. Seems to be old code for TZ. Tailings. An area containing the solid waste material produced in the mining and milling of ore
TZ: New code for TS
UR: Urban

```{r}
fire_spread_veg_data_df$bclcs_level_5_2<-fire_spread_veg_data_df$bclcs_level_5
#Change BP to UR since UR is the updated code for BP
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="BP"] <- "UR"

#Add airport as urban
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="AP"] <- "UR"

#Make various snow/glacier categories into one
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="GL"] <- "SNOW"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="PN"] <- "SNOW"

#Make various rock categories (or road)
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="MI"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="MN"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="MZ"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RZ"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="TA"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="TS"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="TZ"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="OT"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="GP"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="BI"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RN"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="BR"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="MN"] <- "ROCK"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="TA"] <- "ROCK"

#Categories for soil, sediments and mud
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="ES"] <- "SOIL"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="LS"] <- "SOIL"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="MU"] <- "SOIL"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RS"] <- "SOIL"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="LL"] <- "SOIL"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="CB"] <- "SOIL"

#Combine CL and Op categories
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="CL"] <- "OP"

#Categories for water
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="OC"] <- "WATER"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="LA"] <- "WATER"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RE"] <- "WATER"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RI"] <- "WATER"
fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="RM"] <- "WATER"

#For comparison to determine which categories need changing
table(fire_spread_veg_data_df$bclcs_level_5_2) #Burn may be too small. Still 597 without any category for some reason.
#Snow is a very small category. Combine with another category: water?

fire_spread_veg_data_df$bclcs_level_5_2[fire_spread_veg_data_df$bclcs_level_5_2=="SNOW"] <- "WATER"

#Inspect with other divisions
table(fire_spread_veg_data_df$bclcs_level_5_2, fire_spread_veg_data_df$ntrldstrbn)
table(fire_spread_veg_data_df$bclcs_level_5_2, fire_spread_veg_data_df$vegtype2)
#Going to need to remove the burned category for NDT2 and NDT5. Might be ok for NDT3 and NDT4, but still pretty small. 
#Other will need to be removed for NDT4 and NDT5.

```

### Create a wind at fire variable

```{r}
#Create a month variable
fire_spread_veg_data_df$ign_month<-stri_sub(fire_spread_veg_data_df$fire_date,5,6)
table(fire_spread_veg_data_df$ign_month)

#Assign season by month
fire_spread_veg_data_df$wind_atfire<-0
fire_spread_veg_data_df$ign_month<-as.numeric(fire_spread_veg_data_df$ign_month)
head(fire_spread_veg_data_df)
fire_spread_veg_data_df<-fire_spread_veg_data_df %>%
    mutate(wind_atfire = case_when(ign_month == 1 ~ win_spr, #even though not quite accurate
                                  ign_month == 2 ~ win_spr, #even though not quite accurate
                                  ign_month == 3 ~ win_spr,
                                  ign_month == 4 ~ win_spr,
                                  ign_month == 5 ~ win_spr,
                                  ign_month == 6 ~ win_sum,
                                  ign_month == 7 ~ win_sum,
                                  ign_month == 8 ~ win_sum,
                                  ign_month == 9 ~ win_sum,#even though not quite accurate
                                  ign_month == 10 ~ win_sum,#even though not quite accurate
                                  ign_month == 11 ~ win_spr,#even though not quite accurate
                                  ign_month == 12 ~ win_spr,#even though not quite accurate
                                  TRUE ~ win_spr))

fire_spread_veg_data_df$wind_atfire
hist(fire_spread_veg_data_df$wind_atfire)

```

### Create cos transformation for aspect
```{r}
#Before transformations, must convert degrees to radians
fire_spread_veg_data_df$aspect_radians<-(fire_spread_veg_data_df$aspect*pi)/180
hist(fire_spread_veg_data_df$aspect_radians)

fire_spread_veg_data_df$aspect_cos<-cos(fire_spread_veg_data_df$aspect_radians) #try cos because I hypothesize that southern aspects would be most likely to have higher likelihood of fire. 180 degrees is -1 with cos, and north is plus 1.
```

Attempt to save files.

```{r}
write.csv(fire_spread_veg_data_df, file = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\fire_spread_veg_data_.csv")

st_write(fire_spread_veg_data_B, overwrite=TRUE, dsn = "D:\\Fire\\fire_data\\raw_data\\Historical_Fire_Perimiter_polygons\\fire_spread_veg_data.shp", delete_dsn = TRUE)
```

Now that the data frame is prepared, we can move on to data analysis for each NDT.

############################### COMPLETE ######################

Move on to the 15_Fire_Spread_Polygons_Data series
